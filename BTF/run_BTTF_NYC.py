from BTTF import BTTF
import numpy as np
import scipy.io
import pandas as pd
import time
import os
from datetime import datetime


def generate_missing_pattern(dense_tensor, missing_type, missing_rate):
    """
    ç”Ÿæˆä¸åŒç±»å‹çš„ç¼ºå¤±æ¨¡å¼

    å‚æ•°:
    - dense_tensor: åŸå§‹å¼ é‡
    - missing_type: 'RM'(éšæœºç¼ºå¤±), 'NM'(ééšæœºç¼ºå¤±), 'FM'(çº¤ç»´ç¼ºå¤±)
    - missing_rate: ç¼ºå¤±ç‡

    è¿”å›:
    - sparse_tensor: åŒ…å«ç¼ºå¤±çš„å¼ é‡
    """
    dim1, dim2, dim3 = dense_tensor.shape
    sparse_tensor = dense_tensor.copy()

    if missing_type == "RM":
        # éšæœºç¼ºå¤±
        mask = np.random.rand(dim1, dim2, dim3) < missing_rate
        sparse_tensor[mask] = np.nan

    elif missing_type == "NM":
        # ééšæœºç¼ºå¤± - æŒ‰å¤©è¿ç»­ç¼ºå¤±
        print(f"  ç”Ÿæˆééšæœºç¼ºå¤±æ¨¡å¼ (è¿ç»­ç¼ºå¤±å¤©æ•°: {missing_rate * 100:.0f}%)")
        nm_tensor = np.random.rand(dim1, dim2, dim3 // 24)
        binary_tensor = np.zeros((dim1, dim2, dim3))

        for i1 in range(dim1):
            for i2 in range(dim2):
                for i3 in range(nm_tensor.shape[2]):
                    # å†³å®šè¿™ä¸€å¤©æ˜¯å¦å®Œå…¨ç¼ºå¤±
                    if nm_tensor[i1, i2, i3] < missing_rate:
                        binary_tensor[i1, i2, i3 * 24:(i3 + 1) * 24] = 0
                    else:
                        binary_tensor[i1, i2, i3 * 24:(i3 + 1) * 24] = 1

        sparse_tensor[binary_tensor == 0] = np.nan

    elif missing_type == "FM":
        # çº¤ç»´ç¼ºå¤± - æ•´ä¸ªæ—¶é—´ç»´åº¦ç¼ºå¤±
        print(f"  ç”Ÿæˆçº¤ç»´ç¼ºå¤±æ¨¡å¼ (ç¼ºå¤±ä½ç½®: {missing_rate * 100:.0f}%)")
        binary = np.random.rand(dim1, dim2) < missing_rate
        binary_tensor = binary[:, :, np.newaxis] * np.ones((dim1, dim2, dim3))
        sparse_tensor[binary_tensor == 0] = np.nan

    else:
        raise ValueError(f"æœªçŸ¥çš„ç¼ºå¤±ç±»å‹: {missing_type}")

    # ç»Ÿè®¡ç¼ºå¤±ä¿¡æ¯
    total_missing = np.sum(np.isnan(sparse_tensor))
    total_elements = sparse_tensor.size
    actual_missing_rate = total_missing / total_elements

    print(f"  å®é™…ç¼ºå¤±ç‡: {actual_missing_rate:.2%} ({total_missing}/{total_elements})")

    return sparse_tensor


def evaluate_model(model, dense_tensor, sparse_tensor, tensor_hat):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½

    å‚æ•°:
    - model: BTTFæ¨¡å‹å®ä¾‹
    - dense_tensor: åŸå§‹å®Œæ•´å¼ é‡
    - sparse_tensor: åŒ…å«ç¼ºå¤±çš„å¼ é‡
    - tensor_hat: é¢„æµ‹å¼ é‡

    è¿”å›:
    - metrics: åŒ…å«å„ç§è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    # æ‰¾åˆ°æµ‹è¯•ä½ç½®ï¼šåŸå§‹æ•°æ®ä¸ä¸º0ä¸”è¢«æ¨¡å‹å¡«å……çš„ä½ç½®
    pos_test = np.where((np.isnan(sparse_tensor)) & (dense_tensor != 0))

    if len(pos_test[0]) == 0:
        print("  è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æµ‹è¯•ä½ç½®")
        return {
            'mape': np.inf,
            'rmse': np.inf,
            'mae': np.inf,
            'correlation': 0,
            'r2': 0,
            'test_points': 0
        }

    # æå–æµ‹è¯•æ•°æ®
    y_true = dense_tensor[pos_test]
    y_pred = tensor_hat[pos_test]

    # ç¡®ä¿y_trueå’Œy_predæ˜¯1Dæ•°ç»„
    y_true = np.asarray(y_true).flatten()
    y_pred = np.asarray(y_pred).flatten()

    # è¿‡æ»¤æ‰æ— æ•ˆå€¼
    valid_mask = (y_true != 0) & ~np.isnan(y_true) & ~np.isnan(y_pred)
    y_true = y_true[valid_mask]
    y_pred = y_pred[valid_mask]

    if len(y_true) == 0:
        print("  è­¦å‘Š: è¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç‚¹")
        return {
            'mape': np.inf,
            'rmse': np.inf,
            'mae': np.inf,
            'correlation': 0,
            'r2': 0,
            'test_points': 0
        }

    # è®¡ç®—å„ç§æŒ‡æ ‡
    mape = model.compute_mape(y_true, y_pred)
    rmse = model.compute_rmse(y_true, y_pred)
    mae = model.compute_mae(y_true, y_pred)

    # è®¡ç®—ç›¸å…³ç³»æ•°
    if len(y_true) > 1:
        correlation = np.corrcoef(y_true, y_pred)[0, 1]
        if np.isnan(correlation):
            correlation = 0
    else:
        correlation = 0

    # è®¡ç®—RÂ²
    if len(y_true) > 1:
        ss_res = np.sum((y_true - y_pred) ** 2)
        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
        if np.isnan(r2):
            r2 = 0
    else:
        r2 = 0

    metrics = {
        'mape': mape,
        'rmse': rmse,
        'mae': mae,
        'correlation': correlation,
        'r2': r2,
        'test_points': len(y_true)
    }

    return metrics


def run_nyc_experiments_optimized(data_path="./datasets/NYC-data-set/tensor.mat",
                                  output_dir="results",
                                  rank=30,
                                  burn_iter=100,
                                  gibbs_iter=20,
                                  scenarios=None,
                                  time_lags=np.array([1, 2, 24]),
                                  seed=1000):
    """
    è¿è¡Œä¼˜åŒ–çš„NYCæ•°æ®é›†å®éªŒ

    å‚æ•°:
    - data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
    - output_dir: è¾“å‡ºç›®å½•
    - rank: å¼ é‡åˆ†è§£çš„ç§©
    - burn_iter: burn-inè¿­ä»£æ¬¡æ•°
    - gibbs_iter: Gibbsé‡‡æ ·è¿­ä»£æ¬¡æ•°
    - scenarios: å®éªŒåœºæ™¯åˆ—è¡¨
    - time_lags: æ—¶é—´æ»å
    - seed: éšæœºç§å­

    è¿”å›:
    - results_df: ç»“æœDataFrame
    """

    # è®¾ç½®éšæœºç§å­
    np.random.seed(seed)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    # é»˜è®¤åœºæ™¯
    if scenarios is None:
        scenarios = [
            ("RM", 0.4, "Random Missing"),
            ("RM", 0.6, "Random Missing"),
            ("NM", 0.4, "Non-random Missing"),
            ("FM", 0.2, "Fiber Missing")
        ]

    print("=" * 60)
    print("NYC Speed Data - BTTF Experiments")
    print("=" * 60)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æ•°æ®è·¯å¾„: {data_path}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"å‚æ•°è®¾ç½®: rank={rank}, burn_iter={burn_iter}, gibbs_iter={gibbs_iter}")
    print(f"æ—¶é—´æ»å: {time_lags}")
    print("-" * 60)

    # åŠ è½½æ•°æ®
    try:
        print("\nğŸ“‚ åŠ è½½æ•°æ®...")
        dense_tensor = scipy.io.loadmat(data_path)['tensor'].astype(np.float32)
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"   æ•°æ®å½¢çŠ¶: {dense_tensor.shape}")
        print(f"   æ•°æ®èŒƒå›´: [{np.min(dense_tensor):.2f}, {np.max(dense_tensor):.2f}]")
        print(f"   éé›¶å€¼æ¯”ä¾‹: {np.sum(dense_tensor != 0) / dense_tensor.size:.2%}")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

    # å­˜å‚¨ç»“æœ
    all_results = []
    detailed_results = []

    # è¿è¡Œæ¯ä¸ªåœºæ™¯
    for i, (mtype, mrate, mname) in enumerate(scenarios, 1):
        print(f"\nğŸ”¬ åœºæ™¯ {i}/{len(scenarios)}: {mname} ({mrate * 100:.0f}%)")
        print("-" * 40)

        try:
            # ç”Ÿæˆç¼ºå¤±æ¨¡å¼
            sparse_tensor = generate_missing_pattern(dense_tensor, mtype, mrate)

            # åˆå§‹åŒ–æ¨¡å‹
            print("  ğŸš€ åˆå§‹åŒ–BTTFæ¨¡å‹...")
            model = BTTF(dense_tensor, sparse_tensor, time_lags, rank, burn_iter, gibbs_iter)

            # è¿è¡Œé‡‡æ ·
            print("  â³ å¼€å§‹Gibbsé‡‡æ ·...")
            start_time = time.time()
            tensor_hat = model.gibbs_sampling()
            end_time = time.time()
            running_time = end_time - start_time

            # è¯„ä¼°æ¨¡å‹
            print("  ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
            metrics = evaluate_model(model, dense_tensor, sparse_tensor, tensor_hat)

            # è®°å½•ç»“æœ
            result = {
                'Scenario': f"{mname} ({mrate * 100:.0f}%)",
                'Type': mtype,
                'Rate': mrate,
                'Rank': rank,
                'MAPE': metrics['mape'],
                'RMSE': metrics['rmse'],
                'MAE': metrics['mae'],
                'Correlation': metrics['correlation'],
                'R2': metrics['r2'],
                'Test_Points': metrics['test_points'],
                'Time': running_time
            }
            all_results.append(result)

            # è¯¦ç»†ç»“æœ
            detailed_result = result.copy()
            detailed_result.update({
                'Timestamp': datetime.now().isoformat(),
                'Data_Shape': dense_tensor.shape,
                'Burn_Iter': burn_iter,
                'Gibbs_Iter': gibbs_iter,
                'Time_Lags': list(time_lags)
            })
            detailed_results.append(detailed_result)

            # æ‰“å°ç»“æœ
            print(f"\n  ğŸ“ˆ ç»“æœæ‘˜è¦:")
            print(f"     MAPE: {metrics['mape']:.4f}")
            print(f"     RMSE: {metrics['rmse']:.4f}")
            print(f"     MAE: {metrics['mae']:.4f}")
            print(f"     ç›¸å…³ç³»æ•°: {metrics['correlation']:.4f}")
            print(f"     RÂ²: {metrics['r2']:.4f}")
            print(f"     æµ‹è¯•ç‚¹æ•°: {metrics['test_points']}")
            print(f"     è¿è¡Œæ—¶é—´: {running_time:.2f}ç§’")

            # ä¿å­˜å•ä¸ªåœºæ™¯çš„ç»“æœ
            scenario_file = os.path.join(output_dir, f"NYC_{mtype}_{int(mrate * 100)}_results.csv")
            scenario_df = pd.DataFrame([result])
            scenario_df.to_csv(scenario_file, index=False)
            print(f"  ğŸ’¾ åœºæ™¯ç»“æœå·²ä¿å­˜: {scenario_file}")

        except Exception as e:
            print(f"âŒ åœºæ™¯ {mname} è¿è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue

    # ä¿å­˜æ‰€æœ‰ç»“æœ
    if all_results:
        # ç®€æ´ç»“æœ
        results_df = pd.DataFrame(all_results)
        summary_file = os.path.join(output_dir, "NYC_summary_results.csv")
        results_df.to_csv(summary_file, index=False)

        # è¯¦ç»†ç»“æœ
        detailed_df = pd.DataFrame(detailed_results)
        detailed_file = os.path.join(output_dir, "NYC_detailed_results.csv")
        detailed_df.to_csv(detailed_file, index=False)

        # æ‰“å°æ±‡æ€»
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼")
        print("=" * 60)
        print(f"å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æ€»è¿è¡Œæ—¶é—´: {sum(r['Time'] for r in all_results):.2f}ç§’")
        print(f"ç»“æœæ–‡ä»¶:")
        print(f"  - æ±‡æ€»ç»“æœ: {summary_file}")
        print(f"  - è¯¦ç»†ç»“æœ: {detailed_file}")

        # æ€§èƒ½ç»Ÿè®¡
        print("\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"  å¹³å‡MAPE: {results_df['MAPE'].mean():.4f}")
        print(f"  å¹³å‡RMSE: {results_df['RMSE'].mean():.4f}")
        print(f"  æœ€ä½³MAPEåœºæ™¯: {results_df.loc[results_df['MAPE'].idxmin(), 'Scenario']}")
        print(f"  æœ€ä½³RMSEåœºæ™¯: {results_df.loc[results_df['RMSE'].idxmin(), 'Scenario']}")

        return results_df
    else:
        print("\nâŒ æ²¡æœ‰æˆåŠŸå®Œæˆçš„å®éªŒ")
        return None


# å¿«é€Ÿè¿è¡Œå‡½æ•°
def quick_nyc_test():
    """å¿«é€Ÿæµ‹è¯•NYCå®éªŒ"""
    return run_nyc_experiments_optimized(
        burn_iter=50,
        gibbs_iter=10,
        rank=20,
        scenarios=[("RM", 0.4, "Random Missing")]  # åªæµ‹è¯•ä¸€ä¸ªåœºæ™¯
    )


# è¶…å¿«é€Ÿæµ‹è¯•å‡½æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
def debug_nyc_test():
    """è¶…å¿«é€Ÿæµ‹è¯•NYCå®éªŒï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
    return run_nyc_experiments_optimized(
        burn_iter=10,
        gibbs_iter=5,
        rank=10,
        scenarios=[("RM", 0.2, "Random Missing")]  # åªæµ‹è¯•ä¸€ä¸ªåœºæ™¯ï¼Œä½ç¼ºå¤±ç‡
    )


# ä¸»å‡½æ•°
if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´å®éªŒ
    results = run_nyc_experiments_optimized()

    # æˆ–è€…è¿è¡Œå¿«é€Ÿæµ‹è¯•
    # results = quick_nyc_test()

    # æˆ–è€…è¿è¡Œè°ƒè¯•æµ‹è¯•
    # results = debug_nyc_test()
