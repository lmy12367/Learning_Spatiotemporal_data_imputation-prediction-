import time
import numpy as np
from numpy.linalg import inv as inv
from numpy.random import normal as normrnd, multivariate_normal
from scipy.linalg import khatri_rao as kr_prod
from numpy.linalg import solve as solve
from numpy.linalg import cholesky as cholesky_lower
from scipy.linalg import cholesky as cholesky_upper
from scipy.linalg import solve_triangular as solve_ut
from scipy.stats import wishart, invwishart
import warnings

warnings.filterwarnings('ignore')


# ==============================================================================
# --- è¾…åŠ©å‡½æ•° ---
# ==============================================================================
def ten2mat(tensor, mode):
    """å°†å¼ é‡æ²¿æŒ‡å®šæ¨¡å¼å±•å¼€ä¸ºçŸ©é˜µ"""
    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1), order='F')


def cov_mat(mat, mat_bar):
    """è®¡ç®—åæ–¹å·®çŸ©é˜µ"""
    mat = mat - mat_bar
    return mat.T @ mat


def mvnrnd_pre(mu, Lambda):
    """é«˜æ•ˆçš„å¤šå…ƒæ­£æ€åˆ†å¸ƒé‡‡æ ·ï¼Œåˆ©ç”¨Choleskyåˆ†è§£"""
    try:
        # ä½¿ç”¨ä¸Šä¸‰è§’Choleskyåˆ†è§£
        L = cholesky_upper(Lambda, overwrite_a=True, check_finite=False)
        src = normrnd(size=(mu.shape[0],))
        return solve_ut(L, src, lower=False, check_finite=False, overwrite_b=True) + mu
    except np.linalg.LinAlgError:
        # å¦‚æœCholeskyå¤±è´¥ï¼Œä½¿ç”¨SVDåˆ†è§£
        U, s, Vt = np.linalg.svd(Lambda)
        s = np.maximum(s, 1e-10)  # ç¡®ä¿å¥‡å¼‚å€¼ä¸ä¸ºé›¶
        return mu + U @ np.diag(np.sqrt(s)) @ Vt @ np.random.randn(mu.shape[0])


def mnrnd(M, U, V):
    """ç”ŸæˆçŸ©é˜µæ­£æ€åˆ†å¸ƒéšæœºçŸ©é˜µ"""
    dim1, dim2 = M.shape
    try:
        X0 = np.random.randn(dim1, dim2)
        P = cholesky_lower(U)
        Q = cholesky_lower(V)
        return M + P @ X0 @ Q.T
    except:
        # å¦‚æœå¤±è´¥ï¼Œè¿”å›å‡å€¼
        return M


# ==============================================================================
# --- ä¼˜åŒ–ç‰ˆBTTFç±» ---
# ==============================================================================
class BTTF:
    """
    ä¼˜åŒ–ç‰ˆè´å¶æ–¯æ—¶é—´å¼ é‡åˆ†è§£ (Bayesian Temporal Tensor Factorization)

    ä¸»è¦æ”¹è¿›ï¼š
    1. è‡ªåŠ¨æ•°æ®æ ‡å‡†åŒ–
    2. å¢å¼ºçš„æ•°å€¼ç¨³å®šæ€§
    3. æ”¹è¿›çš„æ”¶æ•›æ£€æµ‹
    4. ä¼˜åŒ–çš„è¯„ä¼°æŒ‡æ ‡
    5. æ›´å¥½çš„é”™è¯¯å¤„ç†
    """

    def __init__(self, dense_tensor, sparse_tensor, time_lags, rank, burn_iter=500, gibbs_iter=100):
        """
        åˆå§‹åŒ–BTTFæ¨¡å‹

        å‚æ•°:
        - dense_tensor: å®Œæ•´çš„å¼ é‡æ•°æ®
        - sparse_tensor: åŒ…å«ç¼ºå¤±å€¼çš„å¼ é‡
        - time_lags: æ—¶é—´æ»åæ•°ç»„
        - rank: åˆ†è§£çš„ç§©
        - burn_iter: burn-inè¿­ä»£æ¬¡æ•°
        - gibbs_iter: Gibbsé‡‡æ ·è¿­ä»£æ¬¡æ•°
        """
        self.dense_tensor = dense_tensor.copy()
        self.sparse_tensor = sparse_tensor.copy()
        self.time_lags = np.array(time_lags, dtype=int)
        self.rank = rank
        self.burn_iter = burn_iter
        self.gibbs_iter = gibbs_iter

        self.dim1, self.dim2, self.T = sparse_tensor.shape

        # æ•°æ®æ ‡å‡†åŒ–
        self._normalize_data()

        # åˆå§‹åŒ–å‚æ•°
        self.init_parameters()

        # éªŒè¯é…ç½®
        self._validate_config()

    def _normalize_data(self):
        """æ•°æ®æ ‡å‡†åŒ–"""
        # è®°å½•åŸå§‹æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
        self.original_min = np.nanmin(self.dense_tensor)
        self.original_max = np.nanmax(self.dense_tensor)
        self.original_mean = np.nanmean(self.dense_tensor)

        # é¿å…é™¤é›¶
        if self.original_max > self.original_min:
            self.scaler = self.original_max - self.original_min
            self.dense_tensor = (self.dense_tensor - self.original_min) / self.scaler
            self.sparse_tensor = (self.sparse_tensor - self.original_min) / self.scaler
        else:
            self.scaler = 1.0
            print("è­¦å‘Š: æ•°æ®ä¸ºå¸¸æ•°ï¼Œè·³è¿‡æ ‡å‡†åŒ–")

        # å¤„ç†ç¼ºå¤±å€¼
        self.ind = ~np.isnan(self.sparse_tensor)
        self.sparse_tensor[np.isnan(self.sparse_tensor)] = 0

    def _validate_config(self):
        """éªŒè¯é…ç½®å‚æ•°"""
        if self.T <= np.max(self.time_lags):
            print(f"è­¦å‘Š: æ—¶é—´åºåˆ—é•¿åº¦({self.T})å°äºæœ€å¤§æ—¶é—´æ»å({np.max(self.time_lags)})")

        if self.rank <= 0:
            raise ValueError("Rankå¿…é¡»å¤§äº0")

        if self.burn_iter <= 0 or self.gibbs_iter <= 0:
            raise ValueError("è¿­ä»£æ¬¡æ•°å¿…é¡»å¤§äº0")

    def init_parameters(self):
        """åˆå§‹åŒ–æ¨¡å‹å‚æ•°"""
        # ä½¿ç”¨è¾ƒå°çš„åˆå§‹å€¼ä»¥è·å¾—æ›´å¥½çš„æ”¶æ•›æ€§
        init_scale = 0.01
        self.U = init_scale * np.random.randn(self.dim1, self.rank)
        self.V = init_scale * np.random.randn(self.dim2, self.rank)
        self.X = init_scale * np.random.randn(self.T, self.rank)

        # VARç³»æ•°çŸ©é˜µ
        d = len(self.time_lags)
        self.A = init_scale * np.random.randn(self.rank * d, self.rank)

        # ç²¾åº¦å‚æ•°çŸ©é˜µ
        self.Tau = np.ones((self.dim1, self.dim2))

        # æ”¶æ•›ç›¸å…³å˜é‡
        self.convergence_history = []

    def _sample_factor_u(self):
        """é‡‡æ ·ç©ºé—´å› å­U"""
        # è‡ªé€‚åº”æ­£åˆ™åŒ–
        Lambda_u = (1e-6 / self.dim1) * np.eye(self.rank)

        var1 = kr_prod(self.X, self.V).T
        var2 = kr_prod(var1, var1)
        var3 = (var2 @ ten2mat(self.ind, 0).T).reshape([self.rank, self.rank, self.dim1]) + Lambda_u[..., np.newaxis]

        # ä½¿ç”¨å¹¿æ’­çš„Tau
        tau_ind = self.Tau[:, :, None] * self.ind
        tau_sparse_tensor = self.Tau[:, :, None] * self.sparse_tensor
        var4 = var1 @ ten2mat(tau_sparse_tensor, 0).T

        for i in range(self.dim1):
            try:
                self.U[i, :] = mvnrnd_pre(solve(var3[:, :, i], var4[:, i]), var3[:, :, i])
            except:
                # å¦‚æœé‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨æ­£æ€åˆ†å¸ƒ
                self.U[i, :] = multivariate_normal(np.zeros(self.rank), Lambda_u)

    def _sample_factor_v(self):
        """é‡‡æ ·ç©ºé—´å› å­V"""
        Lambda_v = (1e-6 / self.dim2) * np.eye(self.rank)

        var1 = kr_prod(self.X, self.U).T
        var2 = kr_prod(var1, var1)
        var3 = (var2 @ ten2mat(self.ind, 1).T).reshape([self.rank, self.rank, self.dim2]) + Lambda_v[..., np.newaxis]

        tau_ind = self.Tau[:, :, None] * self.ind
        tau_sparse_tensor = self.Tau[:, :, None] * self.sparse_tensor
        var4 = var1 @ ten2mat(tau_sparse_tensor, 1).T

        for j in range(self.dim2):
            try:
                self.V[j, :] = mvnrnd_pre(solve(var3[:, :, j], var4[:, j]), var3[:, :, j])
            except:
                self.V[j, :] = multivariate_normal(np.zeros(self.rank), Lambda_v)

    def _sample_factor_x(self, Sigma):
        """é‡‡æ ·æ—¶é—´å› å­X"""
        dim3, rank = self.X.shape
        d = len(self.time_lags)
        tmax = np.max(self.time_lags)
        tmin = np.min(self.time_lags)

        # é¢„è®¡ç®—ä¸€äº›çŸ©é˜µ
        A0 = np.dstack([self.A] * d)
        for k in range(d):
            A0[k * rank: (k + 1) * rank, :, k] = 0

        mat0 = inv(Sigma) @ self.A.T
        mat1 = np.einsum('kij, jt -> kit', self.A.reshape([d, rank, rank]), inv(Sigma))
        mat2 = np.einsum('kit, kjt -> ij', mat1, self.A.reshape([d, rank, rank]))

        var1 = kr_prod(self.V, self.U).T
        var2 = kr_prod(var1, var1)
        var3 = (var2 @ ten2mat(self.ind, 2).T).reshape([rank, rank, dim3]) + inv(Sigma)[:, :, None]

        tau_ind = self.Tau[:, :, None] * self.ind
        tau_sparse_tensor = self.Tau[:, :, None] * self.sparse_tensor
        var4 = var1 @ ten2mat(tau_sparse_tensor, 2).T

        for t in range(dim3):
            Mt = np.zeros((rank, rank))
            Nt = np.zeros(rank)
            Qt = mat0 @ self.X[t - self.time_lags, :].reshape(rank * d)

            index = list(range(0, d))
            if t >= dim3 - tmax and t < dim3 - tmin:
                index = list(np.where(t + self.time_lags < dim3))[0]
            elif t < tmax:
                Qt = np.zeros(rank)
                index = list(np.where(t + self.time_lags >= tmax))[0]

            if t < dim3 - tmin:
                Mt = mat2.copy()
                temp = np.zeros((rank * d, len(index)))
                n = 0
                for k in index:
                    temp[:, n] = self.X[t + self.time_lags[k] - self.time_lags, :].reshape(rank * d)
                    n += 1
                temp0 = self.X[t + self.time_lags[index], :].T - np.einsum('ijk, ik -> jk', A0[:, :, index], temp)
                Nt = np.einsum('kij, jk -> i', mat1[index, :, :], temp0)

            var3[:, :, t] = var3[:, :, t] + Mt
            if t < tmax:
                var3[:, :, t] = var3[:, :, t] - inv(Sigma) + np.eye(rank)

            try:
                self.X[t, :] = mvnrnd_pre(solve(var3[:, :, t], var4[:, t] + Nt + Qt), var3[:, :, t])
            except:
                self.X[t, :] = multivariate_normal(np.zeros(rank), Sigma)

    def _sample_var_coefficient(self):
        """é‡‡æ ·VARç³»æ•°Aå’Œåæ–¹å·®Sigma"""
        dim, rank = self.X.shape
        d = len(self.time_lags)
        tmax = np.max(self.time_lags)

        if dim <= tmax:
            return np.eye(rank), np.eye(rank)

        # æ„é€ å›å½’çŸ©é˜µ
        Z_mat = self.X[tmax:, :]
        Q_mat = np.zeros((dim - tmax, rank * d))
        for k in range(d):
            Q_mat[:, k * rank: (k + 1) * rank] = self.X[tmax - self.time_lags[k]: dim - self.time_lags[k], :]

        # è‡ªé€‚åº”æ­£åˆ™åŒ–
        lambda_reg = max(1e-6, 1.0 / (dim - tmax))
        var_Psi0 = Q_mat.T @ Q_mat + lambda_reg * np.eye(rank * d)

        # ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
        eigenvals = np.linalg.eigvals(var_Psi0)
        if np.min(eigenvals) < 1e-8:
            var_Psi0 += (1e-8 - np.min(eigenvals)) * np.eye(rank * d)

        var_Psi = inv(var_Psi0)
        var_M = var_Psi @ Q_mat.T @ Z_mat

        var_S = Z_mat.T @ Z_mat - var_M.T @ var_Psi0 @ var_M
        var_S = var_S + lambda_reg * np.eye(rank)

        # ç¡®ä¿var_Sæ­£å®š
        eigenvals_S = np.linalg.eigvals(var_S)
        if np.min(eigenvals_S) < 1e-8:
            var_S += (1e-8 - np.min(eigenvals_S)) * np.eye(rank)

        # é‡‡æ ·Sigma
        try:
            Sigma = invwishart.rvs(df=rank + dim - tmax, scale=var_S)
        except:
            print("è­¦å‘Š: é€†Wisharté‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨å•ä½çŸ©é˜µ")
            Sigma = np.eye(rank)

        # é‡‡æ ·A
        try:
            cov_A = np.kron(Sigma, var_Psi)
            cov_A += 1e-6 * np.eye(cov_A.shape[0])
            A = multivariate_normal(var_M.ravel(), cov_A)
            A = A.reshape(rank * d, rank)
        except:
            print("è­¦å‘Š: çŸ©é˜µæ­£æ€é‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨å‡å€¼")
            A = var_M.reshape(rank * d, rank)

        return A, Sigma

    def _sample_precision_tau(self):
        """é‡‡æ ·ç²¾åº¦å‚æ•°Tau"""
        tensor_hat = np.einsum('ir, jr, tr -> ijt', self.U, self.V, self.X)

        # ä½¿ç”¨æ¸©å’Œçš„å…ˆéªŒ
        var_alpha = 1e-6 + 0.5 * np.sum(self.ind, axis=2)
        var_beta = 1e-6 + 0.5 * np.sum(((self.sparse_tensor - tensor_hat) ** 2) * self.ind, axis=2)

        # é¿å…é™¤é›¶
        var_beta = np.maximum(var_beta, 1e-10)
        self.Tau = np.random.gamma(var_alpha, 1 / var_beta)

    def _check_convergence(self, it, tensor_hat):
        """æ£€æŸ¥æ”¶æ•›æ€§"""
        if it <= self.burn_iter:
            return False

        # è®¡ç®—å½“å‰è¯¯å·®
        current_error = np.linalg.norm(self.sparse_tensor - tensor_hat)
        self.convergence_history.append(current_error)

        # æ£€æŸ¥æœ€è¿‘å‡ æ¬¡çš„å˜åŒ–
        if len(self.convergence_history) >= 10:
            recent_errors = self.convergence_history[-10:]
            change = np.std(recent_errors) / (np.mean(recent_errors) + 1e-10)

            if change < 1e-4:
                return True

        return False

    def gibbs_sampling(self):
        """æ‰§è¡ŒGibbsé‡‡æ ·"""
        # åˆå§‹åŒ–ç´¯åŠ å™¨
        U_plus = np.zeros_like(self.U)
        V_plus = np.zeros_like(self.V)
        X_plus = np.zeros_like(self.X)
        A_plus = np.zeros_like(self.A)
        tensor_hat_plus = np.zeros_like(self.sparse_tensor)

        total_iter = self.burn_iter + self.gibbs_iter
        start_time = time.time()
        last_print_time = start_time

        print(f"å¼€å§‹Gibbsé‡‡æ ·: {self.burn_iter} burn-in + {self.gibbs_iter} sampling")

        for it in range(total_iter):
            iter_start = time.time()

            # é‡‡æ ·æ­¥éª¤
            self._sample_factor_u()
            self._sample_factor_v()
            self.A, Sigma = self._sample_var_coefficient()
            self._sample_factor_x(Sigma)
            self._sample_precision_tau()

            # è®¡ç®—å½“å‰ä¼°è®¡
            tensor_hat = np.einsum('ir, jr, tr -> ijt', self.U, self.V, self.X)

            # æ£€æŸ¥æ”¶æ•›
            if self._check_convergence(it, tensor_hat):
                elapsed = time.time() - start_time
                print(f"\nğŸ‰ æ¨¡å‹æå‰æ”¶æ•›äºç¬¬ {it + 1} è½®ï¼")
                print(f"   - æ€»ç”¨æ—¶: {elapsed:.1f}ç§’")
                break

            # ç´¯åŠ æ ·æœ¬
            if it + 1 > self.burn_iter:
                U_plus += self.U
                V_plus += self.V
                X_plus += self.X
                A_plus += self.A
                tensor_hat_plus += tensor_hat

            # è¿›åº¦æ˜¾ç¤º
            current_time = time.time()
            if (it + 1) % 50 == 0 or (it + 1) <= 10 or (current_time - last_print_time) > 30:
                iter_time = current_time - iter_start
                elapsed = current_time - start_time
                remaining = (total_iter - it - 1) * (elapsed / (it + 1))
                current_error = np.linalg.norm(self.sparse_tensor - tensor_hat)

                print(f"ğŸ”„ è½®æ¬¡ {it + 1:4d}/{total_iter} | "
                      f"ç”¨æ—¶ {iter_time:.2f}s | "
                      f"ç´¯è®¡ {elapsed:.1f}s | "
                      f"é¢„è®¡å‰©ä½™ {remaining:.0f}s | "
                      f"è¯¯å·® {current_error:.4f}")

                last_print_time = current_time

        # è®¡ç®—åéªŒå‡å€¼
        print("\nğŸ“Š è®¡ç®—åéªŒå‡å€¼...")
        self.U = U_plus / self.gibbs_iter
        self.V = V_plus / self.gibbs_iter
        self.X = X_plus / self.gibbs_iter
        self.A = A_plus / self.gibbs_iter
        tensor_hat = tensor_hat_plus / self.gibbs_iter

        # åæ ‡å‡†åŒ–
        print("ğŸ”„ åæ ‡å‡†åŒ–ç»“æœ...")
        tensor_hat = tensor_hat * self.scaler + self.original_min
        tensor_hat[tensor_hat < 0] = 0  # ç¡®ä¿éè´Ÿ

        total_time = time.time() - start_time
        print(f"âœ… å®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.1f}ç§’")

        return tensor_hat

    def compute_mape(self, var, var_hat):
        """è®¡ç®—MAPEï¼ˆå¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ï¼‰"""
        mask = var != 0
        if np.sum(mask) == 0:
            return np.inf

        ape = np.abs(var[mask] - var_hat[mask]) / var[mask]

        # ä½¿ç”¨æˆªæ–­å¹³å‡å€¼å‡å°‘å¼‚å¸¸å€¼å½±å“
        if len(ape) > 0:
            ape_sorted = np.sort(ape)
            truncate_idx = int(0.95 * len(ape_sorted))
            if truncate_idx > 0:
                return np.mean(ape_sorted[:truncate_idx])

        return np.mean(ape)

    def compute_rmse(self, var, var_hat):
        """è®¡ç®—RMSEï¼ˆå‡æ–¹æ ¹è¯¯å·®ï¼‰"""
        if len(var) == 0:
            return np.inf
        return np.sqrt(np.mean((var - var_hat) ** 2))

    def compute_mae(self, var, var_hat):
        """è®¡ç®—MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰"""
        if len(var) == 0:
            return np.inf
        return np.mean(np.abs(var - var_hat))

    def evaluate(self, dense_tensor, tensor_hat):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        # æ‰¾åˆ°æµ‹è¯•ä½ç½®ï¼ˆåŸå§‹æ•°æ®ä¸­éé›¶ä¸”è¢«å¡«å……çš„ä½ç½®ï¼‰
        sparse_tensor_norm = (self.sparse_tensor - self.original_min) / self.scaler
        pos_test = np.where((np.isnan(sparse_tensor_norm)) & (dense_tensor != 0))

        if len(pos_test[0]) == 0:
            print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æµ‹è¯•ä½ç½®")
            return None, None, None

        mape = self.compute_mape(dense_tensor[pos_test], tensor_hat[pos_test])
        rmse = self.compute_rmse(dense_tensor[pos_test], tensor_hat[pos_test])
        mae = self.compute_mae(dense_tensor[pos_test], tensor_hat[pos_test])

        return mape, rmse, mae

    def get_factor_matrices(self):
        """è·å–å› å­çŸ©é˜µ"""
        return {
            'U': self.U,
            'V': self.V,
            'X': self.X,
            'A': self.A
        }

    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹å‚æ•°"""
        model_data = {
            'U': self.U,
            'V': self.V,
            'X': self.X,
            'A': self.A,
            'Tau': self.Tau,
            'rank': self.rank,
            'time_lags': self.time_lags,
            'scaler': self.scaler,
            'original_min': self.original_min,
            'original_max': self.original_max
        }
        np.savez(filepath, **model_data)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")

    @classmethod
    def load_model(cls, filepath):
        """åŠ è½½æ¨¡å‹å‚æ•°"""
        data = np.load(filepath)
        # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿå®ä¾‹æ¥å­˜å‚¨å‚æ•°
        instance = cls.__new__(cls)
        instance.U = data['U']
        instance.V = data['V']
        instance.X = data['X']
        instance.A = data['A']
        instance.Tau = data['Tau']
        instance.rank = data['rank']
        instance.time_lags = data['time_lags']
        instance.scaler = data['scaler']
        instance.original_min = data['original_min']
        instance.original_max = data['original_max']
        print(f"æ¨¡å‹å·²ä» {filepath} åŠ è½½")
        return instance
